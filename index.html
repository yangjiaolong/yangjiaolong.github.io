<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="keywords" content="Jiaolong Yang,杨蛟龙,homepage,主页,Computer Vision,PhD,Microsoft Research Asia,MSRA,微软亚洲研究院,Go-ICP,GoICP,Registration,3D Reconstruction,3D Face,Face Recognition,Face Synthesis,DiscoFaceGAN,Video Face,Neural Aggregation Network,NAN,Reflection Removal,VirtualCube,GRAM" />
<meta name="description" content="Jiaolong Yang's Homepage"/>
<link rel="icon" type="x-icon" href="favicon.ico" />
<title>Jiaolong Yang (杨蛟龙)'s Homepage</title>
<link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<!--<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"> -->
<script src="jquery-1.10.2.min.js"> 
</script>
<script type="text/javascript">
var visible = new Array();
function show(id, item){
	div = "#"+id+"_"+item;
	for (i=0;i<visible.length;i++){
		//if(visible[i].substr(1,1)==id){
		if(visible[i]==div){
			if(visible[i]==div){
				$(div).hide(200);
				visible.splice(i,1);
			}
			else{
				$(visible[i]).hide(200);
				visible.splice(i,1);
				$(div).show(200);
				visible.push(div);
			}
			return;
		}
	}
	$(div).show(200);
	visible.push(div);
}

function show2(id, item){
	div = "#"+id+"_"+item;
	for (i=0;i<visible.length;i++){
		if(visible[i].substr(1,1)==id){
			if(visible[i]==div){
				$(div).hide(0);
				visible.splice(i,1);
			}
		}
	}
	$(div).show(200);
	visible.push(div);
}

function shownamedesc()
{
	$("#jellon_b_right").toggle(100);
	return false;
}

</script>
<body>

<div id="jellon">

<div id="jellon_basic">

<div id="jellon_b_left"><div id="jellon_b_r_photo"><img src="photo1.jpg" height="170"/></div></div>

<div id="jellon_b_middle">
<span id="jellon_b_l_name">Jiaolong YANG (杨蛟龙)</span> <span style="padding:3px 0px 0px 10px;">(<a href="#" onclick="javascript:shownamedesc();return false;">?</a>)</span><br/>
<span id="jellon_b_l_title">Ph.D. (Computer Science and Engineering)</span>
<p>
Senior Researcher<br />
Visual Computing Group<br />
Microsoft Research Asia (MSRA)<br />
</p>
<span id="jellon_b_l_contact">jiaoyan [at] microsoft.com</span><br />
<a href="https://www.microsoft.com/en-us/research/people/jiaoyan/" target="_blank" style="color:black;text-decoration:underline;line-height:30px">Microsoft Homepage</a>
</div>


<div id="jellon_b_right">
About my name<br /><span style="font-size:10px;line-height:16px;">
Jiaolong (蛟龙): the given name from my parents. "Jiaolong" is an aquatic dragon in Chinese ancient legends with great power. It can be pronounced as "chiao-lung".<br />
Yang (杨): the family name from my forefathers, the sixth most common surname in China. It can be pronounced as the word "young" with a rising tone.
</span>
</div>

</div>

<div id="jellon_detail">
<!-- ITEM BEGIN -->

<div class="jellon_d_item">
<div style="float:left;height:70px;padding-top:33px;padding-right:1px;"><img src="new.jpg" height="35px" width="35px"/></div>
<div style="padding-left:1px;">
<font color="red">
[04/01/2022] <i>2 papers accepted by SIGGRAPH'22: <a style="color:red;text-decoration:underline;" href="#adampi"  onclick="show2('32','abstract');">AdaMPI - Single-photo 3D view synthisis</a> and <a style="color:red;text-decoration:underline;" href="#deformcaricature" onclick="show2('31','abstract');">Deformable 3D caricature faces</a>.</i><br/>
[03/29/2022] <i>Our <a style="color:red;text-decoration:underline;" href="https://yudeng.github.io/GRAM/" target="_blank">GRAM - 3D-Aware GAN</a> paper is accepted by CVPR'22 as Oral presentation.</i><br/>
[03/16/2022] <i>Our <a style="color:red;text-decoration:underline;" href="https://www.microsoft.com/en-us/research/project/virtualcube/" target="_blank">VirtualCube - 3D Video Conferecing</a> paper received <a href="VirtualCube_IEEE_VR22_Best_Paper_Award_Certification.jpg" target="_blank" style="color:red;text-decoration:underline;">IEEE VR'22 Best Paper Award</a>! </i><br/>
<!--[12/17/2021] <i>Checkout our recent exciting projects: <a style="color:red;text-decoration:underline;" href="https://www.microsoft.com/en-us/research/project/virtualcube/" target="_blank">VirtualCube - 3D Video Conferecing</a> and <a style="color:red;text-decoration:underline;" href="https://yudeng.github.io/GRAM/" target="_blank">GRAM - 3D-Aware GAN</a>.</i><br/>-->
<!--[08/02/2021] <i>The <a style="color:red;text-decoration:underline;" href="#1dflow" onclick="show2('26','abstract');">low-light raw image denosing</a> paper is accepted by TPAMI</a>.</i><br/>-->
[07/23/2021] <i>2 papers accepted by ICCV'21: <a style="color:red;text-decoration:underline;" href="#facetexcomp" onclick="show2('26','abstract');">high-res face texture completion</a> and <a style="color:red;text-decoration:underline;" href="#1dflow" onclick="show2('25','abstract');">high-res optical flow estimation</a> (Oral).</i><br/>
[07/20/2021] <i>A <a href="https://github.com/sicxu/Deep3DFaceRecon_pytorch" target="_blank" style="color:red;text-decoration:underline;">PyTorch version</a> of our <a style="color:red;text-decoration:underline;" href="#face3drecon" onclick="show2('16','abstract');">Deep3DFace</a> method is released: more accurate and easier to use!</a></i><br/>
<!--[03/04/2021] <i>The <a style="color:red;text-decoration:underline;" href="#difnet" onclick="show2('22','abstract');">deep deformed implicit filed (DIF-Net) paper</a> is accepted by CVPR'21.</i><br/>-->
<!--[03/13/2020] <i>3 papers accepted by CVPR'20: <a style="color:red;text-decoration:underline;" href="#disenfacegan" onclick="show2('21','abstract');">disentangled face GAN</a> (Oral), <a style="color:red;text-decoration:underline;" href="#deep3dportrait" onclick="show2('20','abstract');">3D portrait reconstrucion</a>, <a style="color:red;text-decoration:underline;" href="#lowlightdenosing" onclick="show2('19','abstract');">low-light denoising</a> (Oral).</i><br/>-->
<!--[07/23/2019] <i>The <a style="color:red;text-decoration:underline;" href="#videofacedeblur" onclick="show2('18','abstract');">3D-aided face video  debluring paper</a> is accepted by ICCV'19 as Oral.</i><br/>-->
<!--[03/03/2019] <i>The <a style="color:red;text-decoration:underline;" href="#ERRNet" onclick="show2('17','abstract');">deep reflection removal paper</a> is accepted by CVPR'19</i><br/>-->
<!--[03/30/2019] <i>The <a style="color:red;text-decoration:underline;" href="#face3drecon" onclick="show2('16','abstract');">3D face reconstruction paper</a> received the Best Paper Award at CVPR'19 workshop on AMFG</i><br/>-->
<!--[07/04/2018] <i>The <a style="color:red;text-decoration:underline;" href="#dnnquantization" onclick="show2('13','abstract');">neural network quantization paper</a> is accepted by ECCV'18</i><br/>-->
<!--[03/08/2018] <i>The <a style="color:red;text-decoration:underline;" href="#deepintrinsicimage" onclick="show2('12','abstract');">intrinsic image decomposition paper</a> is accepted by CVPR'18 as Oral</i><br/>-->
<!--[07/17/2017] <i>The <a style="color:red;text-decoration:underline;" href="#dcnnreflectionsmoothing" onclick="show2('11','abstract');">deep reflection removal paper</a> is accepted by ICCV'17</i><br/>-->
<!--[03/18/2017] <i>The <a style="color:red;text-decoration:underline;" href="#videoface" onclick="show2('9','abstract');">video face recognition paper</a> is accepted by CVPR'17</i><br/>-->
<!--[01/03/2016] <i>The <a style="color:red;text-decoration:underline;" href="#robustopticalflow" onclick="show2('8','abstract');">robust optical flow paper</a> is accepted by CVPR'16</i><br/>-->
<!--[16/02/2016] <i>I will join <a style="color:red;text-decoration:underline;" target="_blank" href="http://research.microsoft.com/en-us/labs/asia/">Microsoft Research Asia (MSRA)</a> as an Associate Researcher</i><br/>-->
<!--[23/11/2015] <i>I joined <a style="color:red;text-decoration:underline;" target="_blank" href="http://research.microsoft.com/en-us/labs/asia/">Microsoft Research Asia (MSRA)</a> as a Research Intern</i><br/>-->
<!--[10/11/2015] <i>The <a style="color:red;text-decoration:underline;" href="#goicpjournal" onclick="show2('7','abstract');">Go-ICP paper</a> is accepted by T-PAMI</i><br/>-->
<!--[02/03/2015] <i>The <a style="color:red;text-decoration:underline;" target="_blank" href="cvpr2015_opticalflow.pdf">optical flow paper</a> is accepted by CVPR'15</i><br/>-->
<!--[05/08/2014] <i>Project page and source code for Go-ICP can be found <a target="_blank" href="./go-icp" style="color:red;text-decoration:underline;">here</a></i><br/>-->
</font>
</div>
</div>
<!-- ITEM END -->

<!-- ITEM BEGIN -->
<div class="jellon_d_item">
<div class="jellon_d_i_title">Bio</div>
<div class="jellon_d_i_content">I'm currently a Senior Researcher in the Microsoft Research Asia (MSRA) Lab located in Beijing, China. I do research in Computer Vision, where my interests include 3D reconstruction, human face &  body rendering, camera and image motion estimation, and low-level vision & image processing. Part of my research has been transfered to various Microsoft Products such as Microsoft Cognitive Services, Windows Hello, Microsoft XiaoIce, etc. I serve regularly
as program committee member/reviewer for major computer vision conferences and journals including CVPR/ICCV/ECCV/TPAMI/IJCV, and am an Area Chair for CVPR’21, ICCV'21, CVPR'22 and WACV'22.
<p>Before joining MSRA in Sep 2016, I received dual PhD degrees from The Australian National University (ANU) and Beijing Institute of Technology (BIT) in 2016. I was a research intern at MSRA from Nov 2015 to Mar 2016, and was an visiting graduate researcher at Harvard University between Jul 2016 and Aug 2016. I received the Excellent PhD Thesis Award from China Society of Image and Graphics (CSIG) in 2017 (4 recipients in China).</p></div>
</div>
<!-- ITEM END -->


<!-- ITEM BEGIN -->
<div class="jellon_d_item">
<div class="jellon_d_i_title">Publications <span style="font-style:normal;font-weight:normal;">(<a target="_blank" href="https://scholar.google.com/citations?user=GuqoolgAAAAJ&hl=en" style="text-decoration:underline;font-size:15px;font-weight:normal;">Google Scholar</a><!--, <a target="_blank" href="full_publications.html" style="text-decoration:underline;font-size:15px;font-weight:normal;">Full List</a>-->)</span></div>
<!--By year: <a href="#" onclick="show('10','bibtex');return false;">2017</a>, <a href="#" onclick="show('10','bibtex');return false;">2016</a>, <a href="#" onclick="show('10','bibtex');return false;">2015</a>, <a href="#" onclick="show('10','bibtex');return false;">2014</a>, <a href="#" onclick="show('10','bibtex');return false;">2013</a>, <a href="#" onclick="show('10','bibtex');return false;">2012</a><br/>
By keywords: <a href="#" onclick="show('10','bibtex');return false;">3D</a>, <a href="#" onclick="show('10','bibtex');return false;">Deep Learning</a>, <a href="#" onclick="show('10','bibtex');return false;">Optical Flow</a>, <a href="#" onclick="show('10','bibtex');return false;">Face</a>, <a href="#" onclick="show('10','bibtex');return false;">Camera Motion</a>, <a href="#" onclick="show('10','bibtex');return false;">Registration</a>, <a href="#" onclick="show('10','bibtex');return false;">Relection Removal</a><br/>-->
<div class="jellon_d_i_content">
<table id="jellon_papertable" style="width:100%;"  border="0" cellpadding="0" cellspacing="0">

<tr>
<td width="132px"><img src="arxiv22_gramhd.png" width="140"/><a name="gramhd" id="gramhd"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
Jianfeng Xiang<sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://yudeng.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Yu Deng</a>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds</span><br/>
<span class="jellon_d_i_c_refer">arXiv:2206.07255, 2022</b></span><br/>
[<a href="#" onclick="show('35','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('35','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2206.07255.pdf">PDF</a>] [<a target="_blank" href="#" onclick="alert('Available soon');return false;">Code</a>] [<a target="_blank" href="https://jeffreyxiang.github.io/GRAM-HD/">Webpage</a>] [<a target="_blank" href="https://arxiv.org/pdf/2206.07255.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2206.07255">arXiv</a>]<br />
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="35_abstract">
Recent works have shown that 3D-aware GANs trained on unstructured single image collections can generate multiview images of novel instances. The key underpinnings to achieve this are a 3D radiance field generator and a volume rendering process. However, existing methods either cannot generate high-resolution images (e.g., up to 256X256) due to the high computation cost of neural volume rendering, or rely on 2D CNNs for image-space upsampling which jeopardizes the 3D consistency across different views. This paper proposes a novel 3D-aware GAN that can generate high resolution images (up to 1024X1024) while keeping strict 3D consistency as in volume rendering. Our motivation is to achieve super-resolution directly in the 3D space to preserve 3D consistency. We avoid the otherwise prohibitively-expensive computation cost by applying 2D convolutions on a set of 2D radiance manifolds defined in the recent generative radiance manifold (GRAM) approach, and apply dedicated loss functions for effective GAN training at high resolution. Experiments on FFHQ and AFHQv2 datasets show that our method can produce high-quality 3D-consistent results that significantly outperform existing methods. Videos can be found on the project page.
</div>
<div id="35_bibtex">
@inproceedings{xiang2022gramhd,<br/>
&nbsp;&nbsp;author = {Xiang, Jianfeng and Yang, Jiaolong and Deng, Yu and Tong, Xin},<br/>
&nbsp;&nbsp;title = {GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds},<br/>
&nbsp;&nbsp;booktitle = {arXiv:2206.07255},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2022}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="tpami22_mpsnerf.png" width="140"/><a name="mpsnerf" id="mpsnerf"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
Xiangjun Gao<sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span><sup>$</sup>, Jongyoo Kim, <a href="https://pengsida.net/" target="_blank" style="text-decoration:underline;color:#000;">Sida Peng</a>, <a href="https://www.microsoft.com/en-us/research/people/zliu/" target="_blank" style="text-decoration:underline;color:#000;">Zicheng Liu</a>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images</span><br/>
<span class="jellon_d_i_c_refer">IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2022</span><br/>
[<a href="#" onclick="show('34','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('34','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2203.16875.pdf">PDF</a>] [<a target="_blank" href="#" onclick="alert('Available soon');return false;">Code</a>] [<a target="_blank" href="https://gaoxiangjun.github.io/mps_nerf/">Webpage</a>] [<a target="_blank" href="https://arxiv.org/pdf/2203.16875.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2203.16875">arXiv</a>]<br />
(<sup>+</sup>: Intern at MSRA. <sup>$</sup>: Corresponding author)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="34_abstract">
There has been rapid progress recently on 3D human rendering, including novel view synthesis and pose animation, based on the advances of neural radiance fields (NeRF). However, most existing methods focus on person-specific training and their training typically requires multi-view videos. This paper deals with a new challenging task – rendering novel views and novel poses for a person unseen in training, using only multiview still images as input without videos. For this task, we propose a simple yet surprisingly effective method to train a generalizable NeRF with multiview images as conditional input. The key ingredient is a dedicated representation combining a canonical NeRF and a volume deformation scheme. Using a canonical space enables our method to learn shared properties of human and easily generalize to different people. Volume deformation is used to connect the canonical space with input and target images and query image features for radiance and density prediction. We leverage the parametric 3D human model fitted on the input images to derive the deformation, which works quite well in practice when combined with our canonical NeRF. The experiments on both real and synthetic data with the novel view synthesis and pose animation tasks ollectively demonstrate the efficacy of our method.
</div>
<div id="34_bibtex">
@inproceedings{deng2021gram,<br/>
&nbsp;&nbsp;author = {Gao, Xiangjun and Yang, Jiaolong and Kim, Jongyoo and Peng, Sida and Liu, Zicheng and Tong, Xin},<br/>
&nbsp;&nbsp;title = {MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images},<br/>
&nbsp;&nbsp;booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence (to appear)},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2022}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="eccv22_pgmpi.png" width="140"/><a name="mpsnerf" id="mpsnerf"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://ken-ouyang.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Hao Ouyang</a>, <a href="https://bo-zhang.me/" target="_blank" style="text-decoration:underline;color:#000;">Bo Zhang</a>, <a href="https://panzhang0212.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Pan Zhang</a>, <a href="https://www.haya.pro" target="_blank" style="text-decoration:underline;color:#000;">Hao Yang</a>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://www.dongchen.pro" target="_blank" style="text-decoration:underline;color:#000;">Dong Chen</a>, <a href="https://cqf.io" target="_blank" style="text-decoration:underline;color:#000;">Qifeng Chen</a>, <a href="https://www.microsoft.com/en-us/research/people/fangwen/" target="_blank" style="text-decoration:underline;color:#000;">Fang Wen</a><br/>
<span class="jellon_d_i_c_name">Real-Time Neural Character Rendering with Pose-Guided Multiplane Images</span><br/>
<span class="jellon_d_i_c_refer">The 17th European Conference on Computer Vision<b> (ECCV2022)</b></span><br/>
[<a href="#" onclick="show('33','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('33','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2204.11820.pdf">PDF</a>] [<a target="_blank" href="https://ken-ouyang.github.io/cmpi/index.html">Code</a>] [<a target="_blank" href="https://ken-ouyang.github.io/cmpi/index.html">Webpage</a>] [<a target="_blank" href="https://arxiv.org/pdf/2204.11820.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2204.11820">arXiv</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="33_abstract">
We propose pose-guided multiplane image (MPI) synthesis which can render an animatable character in real scenes with photorealistic quality. We use a portable camera rig to capture the multi-view images along with the driving signal for the moving subject. Our method generalizes the image-to-image translation paradigm, which translates the human pose to a 3D scene representation --- MPIs that can be rendered in free viewpoints, using the multi-views captures as supervision. To fully cultivate the potential of MPI, we propose depth-adaptive MPI which can be learned using variable exposure images while being robust to inaccurate camera registration. Our method demonstrates advantageous novel-view synthesis quality over the state-of-the-art approaches for characters with challenging motions. Moreover, the proposed method is generalizable to novel combinations of training poses and can be explicitly controlled. Our method achieves such expressive and animatable character rendering all in real time, serving as a promising solution for practical applications.
</div>
<div id="33_bibtex">
@inproceedings{hao2022real,<br/>
&nbsp;&nbsp;author = {Ouyang, Hao and Zhang, Bo and Zhang, Pan and Yang, Hao and Yang, Jiaolong and Chen, Dong and Chen, Qifeng and Wen, Fang},<br/>
&nbsp;&nbsp;title = {Real-Time Neural Character Rendering with Pose-Guided Multiplane Images},<br/>
&nbsp;&nbsp;booktitle = {European Conference on Computer Vision},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2022}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="siggraph22_adampi.png" width="140"/><a name="adampi" id="adampi"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
Yuxuan Han<sup>+</sup>, Ruicheng Wang<sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span><sup>$</sup><br/>
<span class="jellon_d_i_c_name">Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images</span><br/>
<span class="jellon_d_i_c_refer">ACM <b>SIGGRAPH 2022</b></span><br/>
[<a href="#" onclick="show('32','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('32','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2205.11733.pdf">PDF</a>] [<a target="_blank" href="https://github.com/yxuhan/AdaMPI">Code</a>] [<a target="_blank" href="https://yxuhan.github.io/AdaMPI/">Webpage</a>] [<a target="_blank" href="https://arxiv.org/pdf/2205.11733.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2205.11733">arXiv</a>]<br />
(<sup>+</sup>: Intern at MSRA. <sup>$</sup>: Corresponding author)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="32_abstract">
This paper deals with the challenging task of synthesizing novel views for in-the-wild photographs. Existing methods have shown promising results leveraging monocular depth estimation and color inpainting with layered depth representations. However, these methods still have limited capability to handle scenes with complex 3D geometry. We propose a new method based on the multiplane image (MPI) representation. To accommodate diverse scene layouts in the wild and tackle the difficulty in producing high-dimensional MPI contents, we design a network structure that consists of two novel modules, one for plane depth adjustment and another for depth-aware radiance prediction. The former adjusts the initial plane positions using the RGBD context feature and an attention mechanism. Given adjusted depth values, the latter predicts the color and density for each plane separately with proper inter-plane interactions achieved via a feature masking strategy. To train our method, we construct large-scale stereo training data using only unconstrained single-view image collections by a simple yet effective warp-back strategy. The experiments on both synthetic and real datasets demonstrate that our trained model works remarkably well and achieves state-of-the-art results.
</div>
<div id="32_bibtex">
@inproceedings{han2022single,<br/>
&nbsp;&nbsp;author = {Han, Yuxuan and Wang, Ruicheng and Yang, Jiaolong},<br/>
&nbsp;&nbsp;title = {Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images},<br/>
&nbsp;&nbsp;booktitle = {ACM SIGGRAPH},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2022}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="siggraph22_3dcaricature.png" width="140"/><a name="deformcaricature" id="deformcaricature"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://ycjung.info/" target="_blank" style="text-decoration:underline;color:#000;">Yucheol Jung</a>, Wonjong Jang, Soongjin Kim, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a>, <a href="http://phome.postech.ac.kr/~leesy/" target="_blank" style="text-decoration:underline;color:#000;">Seungyong Lee</a><br/>
<span class="jellon_d_i_c_name">Deep Deformable 3D Caricatures with Learned Shape Control</span><br/>
<span class="jellon_d_i_c_refer">ACM <b>SIGGRAPH 2022</b></span><br/>
[<a href="#" onclick="show('31','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('31','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2207.14593.pdf">PDF</a>] [<a target="_blank" href="https://github.com/ycjungSubhuman/DeepDeformable3DCaricatures">Code</a>] [<a target="_blank" href="https://ycjungsubhuman.github.io/DeepDeformable3DCaricatures/">Webpage</a>] [<a target="_blank" href="https://ycjungsubhuman.github.io/DeepDeformable3DCaricatures/">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2207.14593">arXiv</a>]<br />
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="31_abstract">
A 3D caricature is an exaggerated 3D depiction of a human face. The goal of this paper is to model the variations of 3D caricatures in a compact parameter space so that we can provide a useful data-driven toolkit for handling 3D caricature deformations. To achieve the goal, we propose an MLP-based framework for building a deformable surface model, which takes a latent code and produces a 3D surface. In the framework, a SIREN MLP models a function that takes a 3D position on a fixed template surface and returns a 3D displacement vector for the input position. We create variations of 3D surfaces by learning a hypernetwork that takes a latent code and produces the parameters of the MLP. Once learned, our deformable model provides a nice editing space for 3D caricatures, supporting label-based semantic editing and point-handle-based deformation, both of which produce highly exaggerated and natural 3D caricature shapes. We also demonstrate other applications of our deformable model, such as automatic 3D caricature creation.
</div>
<div id="31_bibtex">
@inproceedings{deng2021gram,<br/>
&nbsp;&nbsp;author = {Jung, Yucheol and Jang, Wonjong and Kim, Soongjin and Yang, Jiaolong and Tong, Xin and Lee, Seungyong},<br/>
&nbsp;&nbsp;title = {Deep Deformable 3D Caricatures with Learned Shape Control},<br/>
&nbsp;&nbsp;booktitle = {ACM SIGGRAPH},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2022}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="arxiv21_virtualcube.png" width="140"/><a name="virtualcube" id="virtualcube"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<a href="https://yizhongzhang1989.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Yizhong Zhang</a>*, <span class="jellon_d_i_c_emphasize">Jiaolong Yang*</span>, Zhen Liu, Ruicheng Wang, Guojun Chen, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a>, <a href="https://www.microsoft.com/en-us/research/people/bainguo/" target="_blank" style="text-decoration:underline;color:#000;">Baining Guo</a><br/>
<span class="jellon_d_i_c_name">VirtualCube: An Immersive 3D Video Communication System</span><br/>
<span class="jellon_d_i_c_refer">IEEE Conference on Virtual Reality and 3D User Interfaces <b>(VR2022)</b> (& <b>IEEE TVCG</b>)</span> (<span style="color:#e60000"><b><a href="VirtualCube_IEEE_VR22_Best_Paper_Award_Certification.jpg" target="_blank" style="color:#e60000;text-decoration:underline;">Best Journal Paper Award</a>.</b> Check our project webpage!</span>)<br/>
[<a href="#" onclick="show('30','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('30','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2112.06730.pdf">PDF</a>] [<a target="_blank" href="https://www.microsoft.com/en-us/research/project/virtualcube/">Webpage</a>] [<a target="_blank" href="https://arxiv.org/abs/2112.06730">arXiv</a>]<br />
(*: Equal contributions)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="30_abstract">
The VirtualCube system is a 3D video conference system that attempts to overcome some limitations of conventional technologies. The key ingredient is VirtualCube, an abstract representation of a real-world cubicle instrumented with RGBD cameras for capturing the 3D geometry and texture of a user. We design VirtualCube so that the task of data capturing is standardized and significantly simplified, and everything can be built using off-the-shelf hardware. We use VirtualCubes as the basic building blocks of a virtual conferencing environment, and we provide each VirtualCube user with a surrounding display showing life-size videos of remote participants. To achieve real-time rendering of remote participants, we develop the V-Cube View algorithm, which uses multi-view stereo for more accurate depth estimation and Lumi-Net rendering for better rendering quality. The VirtualCube system correctly preserves the mutual eye gaze between participants, allowing them to establish eye contact and be aware of who is visually paying attention to them. The system also allows a participant to have side discussions with remote participants as if they were in the same room. Finally, the system sheds lights on how to support the shared space of work items (e.g., documents and applications) and track the visual attention of participants to work items.
</div>
<div id="30_bibtex">
@article{zhang2021virtualcube,<br/>
&nbsp;&nbsp;author = {Zhang, Yizhong and Yang, Jiaolong and Liu, Zhen and Wang, Ruicheng and Chen, Guojun and Tong, Xin and Guo, Baining},<br/>
&nbsp;&nbsp;title = {VirtualCube: An Immersive 3D Video Communication System},<br/>
&nbsp;&nbsp;booktitle = {IEEE Transactions on Visualization and Computer Graphics},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2022}<br/>
}
</div>
</td>
</tr>


<tr>
<td width="132px"><img src="arxiv21_gram.png" width="140"/><a name="gram" id="gram"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://yudeng.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Yu Deng</a><sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, Jianfeng Xiang, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation</span><br/>
<span class="jellon_d_i_c_refer">The 38th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2022)</b></span> (<b><span style="color:#e60000">Oral</span></b>)<br/>
[<a href="#" onclick="show('29','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('29','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2112.08867.pdf">PDF</a>] [<a target="_blank" href="#" onclick="alert('Available soon');return false;">Code</a>] [<a target="_blank" href="https://yudeng.github.io/GRAM/">Webpage</a>] [<a target="_blank" href="https://arxiv.org/pdf/2112.08867.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2112.08867">arXiv</a>]<br />
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="29_abstract">
3D-aware image generative modeling aims to generate 3D-consistent images with explicitly controllable camera poses. Recent works have shown promising results by training neural radiance field (NeRF) generators on unstructured 2D images, but still can not generate highly-realistic images with fine details. A critical reason is that the high memory and computation cost of volumetric representation learning greatly restricts the number of point samples for radiance integration during training. Deficient sampling not only limits the expressive power of the generator to handle fine details but also impedes effective GAN training due to the noise caused by unstable Monte Carlo sampling. We propose a novel approach that regulates point sampling and radiance field learning on 2D manifolds, embodied as a set of learned implicit surfaces in the 3D volume. For each viewing ray, we calculate ray-surface intersections and accumulate their radiance generated by the network. By training and rendering such radiance manifolds, our generator can produce high quality images with realistic fine details and strong visual 3D consistency.
</div>
<div id="29_bibtex">
@inproceedings{deng2021gram,<br/>
&nbsp;&nbsp;author = {Deng, Yu and Yang, Jiaolong and Xiang, Jianfeng and Tong, Xin},<br/>
&nbsp;&nbsp;title = {GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2022}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="tpami21_facerestoration.png" width="140"/>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
Xiaobin Hu, <a href="https://ying-fu.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Wenqi Ren</a>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://xiaochun-cas.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Xiaochun Cao</a>, <a href="http://www.davidwipf.com/" target="_blank" style="text-decoration:underline;color:#000;">David Wipf</a>, Bjoern Menze, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a>, <a href="https://eecs.pku.edu.cn/info/1505/6625.htm" target="_blank" style="text-decoration:underline;color:#000;">Hongbin Zha</a><br/>
<span class="jellon_d_i_c_name">Face Restoration via Plug-and-Play 3D Facial Priors</span><br/>
<span class="jellon_d_i_c_refer">IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2021</span><br/>
[<a href="#" onclick="show('28','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('28','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2108.02158.pdf">PDF</a>] [<a target="_blank" href="https://github.com/Vandermode/ELD">Code</a>] [<a target="_blank" href="https://arxiv.org/pdf/2108.02158.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2108.02158">arXiv</a>]<br />
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="28_abstract">
State-of-the-art face restoration methods employ deep convolutional neural networks (CNNs) to learn a mapping between degraded and sharp facial patterns by exploring local appearance knowledge. However, most of these methods do not well exploit facial structures and identity information, and only deal with task-specific face restoration (e.g., face super-resolution or deblurring). In this paper, we propose cross-tasks and cross-models plug-and-play 3D facial priors to explicitly embed the network with the sharp facial structures for general face restoration tasks. Our 3D priors are the first to explore 3D morphable knowledge based on the fusion of parametric descriptions of face attributes (e.g., identity, facial expression, texture, illumination, and face pose). Furthermore, the priors can easily be incorporated into any network and are very efficient in improving the performance and accelerating the convergence speed. Firstly, a 3D face rendering branch is set up to obtain 3D priors of salient facial structures and identity knowledge. Secondly, for better exploiting this hierarchical information (i.e., intensity similarity, 3D facial structure, and identity content), a spatial attention module is designed for the image restoration problems. Extensive face restoration experiments including face super-resolution and deblurring demonstrate that the proposed 3D priors achieve superior face restoration results over the state-of-the-art algorithms
</div>
<div id="28_bibtex">
@article{hu2021face,<br/>
&nbsp;&nbsp;author = {Hu, Xiaobin and Ren, Wenqi and Yang, Jiaolong and Cao, Xiaochun and Wipf, David and Menze, Bjoern and Xin, Tong and Zha, Hongbin},<br/>
&nbsp;&nbsp;title = {Face Restoration via Plug-and-Play 3D Facial Priors},<br/>
&nbsp;&nbsp;booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2021}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="tpami21_lowlightdenosing.png" width="140"/>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://kxwei.net/" target="_blank" style="text-decoration:underline;color:#000;">Kaixuan Wei</a>, <a href="https://ying-fu.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Ying Fu</a>, Yinqiang Zheng, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span><br/>
<span class="jellon_d_i_c_name">Physics-based Noise Modeling for Extreme Low-light Photography</span><br/>
<span class="jellon_d_i_c_refer">IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2021</span><br/>
[<a href="#" onclick="show('27','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('27','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2108.02158.pdf">PDF</a>] [<a target="_blank" href="https://github.com/Vandermode/ELD">Code</a>] [<a target="_blank" href="https://arxiv.org/pdf/2108.02158.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2108.02158">arXiv</a>]<br />
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="27_abstract">
Enhancing the visibility in extreme low-light environments is a challenging task. Under nearly lightless condition, existing image denoising methods could easily break down due to significantly low SNR. In this paper, we systematically study the noise statistics in the imaging pipeline of CMOS photosensors, and formulate a comprehensive noise model that can accurately characterize the real noise structures. Our novel model considers the noise sources caused by digital camera electronics which are largely overlooked by existing methods yet have significant influence on raw measurement in the dark. It provides a way to decouple the intricate noise structure into different statistical distributions with physical interpretations. Moreover, our noise model can be used to synthesize realistic training data for learning-based low-light denoising algorithms. In this regard, although promising results have been shown recently with deep convolutional neural networks, the success heavily depends on abundant noisy-clean image pairs for training, which are tremendously difficult to obtain in practice. Generalizing their trained models to images from new devices is also
problematic. Extensive experiments on multiple low-light denoising datasets – including a newly collected one in this work covering various devices – show that a deep neural network trained with our proposed noise formation model can reach surprisingly-high accuracy. The results are on par with or sometimes even outperform training with paired real data, opening a new door to real-world extreme low-light photography.
</div>
<div id="27_bibtex">
@article{wei2021physics,<br/>
&nbsp;&nbsp;author = {Wei, Kaixuan and Fu, Ying and Zheng, Yinqiang and Yang, Jiaolong},<br/>
&nbsp;&nbsp;title = {Physics-based Noise Modeling for Extreme Low-light Photography},<br/>
&nbsp;&nbsp;booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2021}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="iccv21_facetexcomp.png" width="140"/><a name="facetexcomp" id="facetexcomp"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
Jongyoo Kim, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;"> Xin Tong</a><br/>
<span class="jellon_d_i_c_name">Learning High-Fidelity Face Texture Completion without Complete Face Texture</span><br/>
<span class="jellon_d_i_c_refer">The 18th International Conference on Computer Vision <b>(ICCV2021)</b></span><br/>
[<a href="#" onclick="show('26','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('26','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Learning_High-Fidelity_Face_Texture_Completion_Without_Complete_Face_Texture_ICCV_2021_paper.pdf">PDF</a>] [<a target="_blank" href="#" onclick="alert('Available soon');return false;">Code</a>] [<a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Kim_Learning_High-Fidelity_Face_ICCV_2021_supplemental.pdf">Suppl. Material</a>] [<a target="_blank" href="#" onclick="return false;">arXiv</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="26_abstract">
For face texture completion, previous methods typically use some complete textures captured by multiview imaging systems or 3D scanners for supervised learning. This paper deals with a new challenging problem – learning to complete invisible texture in a single face image without using any complete texture. We simply leverage a large corpus of face images of different subjects (e. g., FFHQ) to train a texture completion model in an unsupervised manner. To achieve this, we propose DSD-GAN, a novel deep neural network based method that applies two discriminators in UV map space and image space. These two discriminators work in a complementary manner to learn both facial structures and texture details. We show that their combination is essential to obtain high-fidelity results. Despite the network never sees any complete facial appearance, it is able to generate compelling full textures from single images.
</div>
<div id="26_bibtex">
@inproceedings{kim2021learning,<br/>
&nbsp;&nbsp;author = {Kim, Jongyoo and Yang, Jiaolong and Tong, Xin},<br/>
&nbsp;&nbsp;title = {Learning High-Fidelity Face Texture Completion without Complete Face Texture},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2021}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="iccv21_1dflow.png" width="140"/><a name="1dflow" id="1dflow"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
Haofei Xu<sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://jianfei-cai.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Jianfei Cai</a>, <a href="http://staff.ustc.edu.cn/~juyong/" target="_blank" style="text-decoration:underline;color:#000;">Juyong Zhang</a>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">High-Resolution Optical Flow from 1D Attention and Correlation</span><br/>
<span class="jellon_d_i_c_refer">The 18th International Conference on Computer Vision <b>(ICCV2021)</b></span> (<b><span style="color:#e60000">Oral</span></b>)<br/>
[<a href="#" onclick="show('25','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('25','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2104.13918.pdf">PDF</a>] [<a target="_blank" href="https://github.com/haofeixu/flow1d">Code</a>] [<a target="_blank" href="https://arxiv.org/pdf/2104.13918.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2104.13918">arXiv</a>]<br />
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="25_abstract">
Optical flow is inherently a 2D search problem, and thusthe computational complexity grows quadratically with respect to the search window, making large displacements matching infeasible for high-resolution images.  In this pa-per,  we propose a new method for high-resolution opticalflow estimation with significantly less computation,  whichis achieved by factorizing 2D optical flow with 1D attentionand correlation.  Specifically, we first perform a 1D atten-tion operation in the vertical direction of the target image,and then a simple 1D correlation in the horizontal direc-tion of the attended image can achieve 2D correspondencemodeling effect. The directions of attention and correlationcan also be exchanged,  resulting in two 3D cost volumesthat are concatenated for optical flow estimation. The novel1D formulation empowers our method to scale to very high-resolution input images while maintaining competitive per-formance. Extensive experiments on Sintel, KITTI and real-world 4K (2160×3840) resolution images demonstrated theeffectiveness and superiority of our proposed method.
</div>
<div id="25_bibtex">
@inproceedings{xu2021high,<br/>
&nbsp;&nbsp;author = {Xu, Haofei and Yang, Jiaolong and Cai, Jianfei and Zhang, Juyong and Tong, Xin},<br/>
&nbsp;&nbsp;title = {High-Resolution Optical Flow from 1D Attention and Correlation},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2021}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="ijcai21_IALS.png" width="140"/><a name="IALS" id="IALS"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
Yuxuan Han, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://ying-fu.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Ying Fu</a><br/>
<span class="jellon_d_i_c_name">Disentangled Face Attribute Editing via Instance-Aware Latent Space Search</span><br/>
<span class="jellon_d_i_c_refer">The 30th International Joint Conference on Artificial Intelligence <b>(IJCAI2021)</b></span><br/>
[<a href="#" onclick="show('24','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('24','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2105.12660.pdf">PDF</a>] [<a target="_blank" href="https://github.com/yxuhan/IALS">Code</a>] [<a target="_blank" href="https://arxiv.org/pdf/2105.12660.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2105.12660">arXiv</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="24_abstract">
Recent works have shown that a rich set of semantic directions exist in the latent space of Generative Adversarial Networks (GANs), which enables various facial attribute editing applications. However, existing methods may suffer poor attribute variation disentanglement, leading to unwanted change of other attributes when altering the desired one. The semantic directions used by existing methods are at attribute level, which are difficult to model complex attribute correlations, especially in the presence of attribute distribution bias in GAN’s training set. In this paper, we propose a novel framework (IALS) that performs Instance-Aware Latent-Space Search to find semantic directions for disentangled attribute editing. The instance information is injected by leveraging the supervision from a set of attribute classifiers evaluated on the input images. We further propose a Disentanglement-Transformation (DT) metric to quantify the attribute transformation and disentanglement efficacy and find the optimal control factor between attribute-level and instance-specific directions based on it. Experimental results on both GAN-generated and real-world images collectively show that our method outperforms state-of-the-art methods proposed recently by a wide margin.
</div>
<div id="24_bibtex">
@inproceedings{han2021disentangled,<br/>
&nbsp;&nbsp;author = {Han, Yuxuan and Yang, Jiaolong and Fu, Ying},<br/>
&nbsp;&nbsp;title = {Disentangled Face Attribute Editing via Instance-Aware Latent Space Search},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2021}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="tog21_thumbnail.jpg" width="140"/><a name="stylecarigan" id="stylecarigan"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
Wongjong Jang, Gwangjin Ju, <a href="https://ycjung.info/" target="_blank" style="text-decoration:underline;color:#000;">Yucheol Jung</a>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a>, <a href="http://phome.postech.ac.kr/~leesy/" target="_blank" style="text-decoration:underline;color:#000;">Seungyong Lee</a><br/>
<span class="jellon_d_i_c_name">StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation</span><br/>
<span class="jellon_d_i_c_refer">ACM Transactions on Graphics <b>(TOG)</b>, 2021 (Proc. <b>SIGGRAPH2021</b>) </span><br/>
[<a href="#" onclick="show('23','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('23','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2107.04331.pdf">PDF</a>] [<a target="_blank" href="https://github.com/PeterZhouSZ/StyleCariGAN">Code</a>] [<a target="_blank" href="http://arxiv.org/abs/2107.04331">Suppl. Material</a>] [<a target="_blank" href="http://arxiv.org/abs/2107.04331">arXiv</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="23_abstract">
We present a caricature generation framework based on shape and style manipulation using StyleGAN. Our framework, dubbed StyleCariGAN, automatically creates a realistic and detailed caricature from an input photo with optional controls on shape exaggeration degree and color stylization type. The key component of our method is shape exaggeration blocks that are used for modulating coarse layer feature maps of StyleGAN to produce desirable caricature shape exaggerations. We first build a layer-mixed StyleGAN for photo-to-caricature style conversion by swapping fine layers of the StyleGAN for photos to the corresponding layers of the StyleGAN trained to generate caricatures. Given an input photo, the layer-mixed model produces detailed color stylization for a caricature but without shape exaggerations. We then append shape exaggeration blocks to the coarse layers of the layer-mixed model and train the blocks to create shape exaggerations while preserving the characteristic appearances of the input. Experimental results show that our StyleCariGAN generates realistic and detailed caricatures compared to the current state-of-the-art methods. We demonstrate StyleCariGAN also supports other StyleGAN-based image manipulations, such as facial expression control.
</div>
<div id="23_bibtex">
@article{jang2021stylecarigan,<br/>
&nbsp;&nbsp;author = {Jang, Wongjong and Ju, Gwangjin and Jung, Yucheol and Yang, Jiaolong and Xin, Tong and Lee, Seungyong},<br/>
&nbsp;&nbsp;title = {StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation},<br/>
&nbsp;&nbsp;journal = {ACM Transactions on Graphics},<br/>
<!--&nbsp;&nbsp;pages = {8178-8187},<br/>-->
&nbsp;&nbsp;year = {2021}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr21_difnet.png" width="130"/><a name="difnet" id="difnet"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<a href="https://yudeng.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Yu Deng</a><sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence</span><br/>
<span class="jellon_d_i_c_refer">The 37th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2021)</b></span><br/>
[<a href="#" onclick="show('22','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('22','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2011.13650.pdf">PDF</a>] [<a target="_blank" href="https://github.com/microsoft/DIF-Net">Code</a>] [<a target="_blank" href="https://arxiv.org/pdf/2011.13650.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2011.13650">arXiv</a>]<br/>
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="22_abstract">
We propose a novel Deformed Implicit Field (DIF) representation for modeling 3D shapes of a category and generating dense correspondences among shapes. With DIF, a 3D shape is represented by a template implicit field shared across the category, together with a 3D deformation field and a correction field dedicated for each shape instance. Shape correspondences can be easily established using their deformation fields. Our neural network, dubbed DIF-Net, jointly learns a shape latent space and these fields for 3D objects belonging to a category without using any correspondence or part label. The learned DIF-Net can also provides reliable correspondence uncertainty measurement reflecting shape structure discrepancy. Experiments show that DIF-Net not only produces high-fidelity 3D shapes but also builds high-quality dense correspondences across different shapes. We also demonstrate several applications such as texture transfer and shape editing, where our method achieves compelling results that cannot be achieved by previous methods. 
</div>
<div id="22_bibtex">
@inproceedings{deng2020deformed,<br/>
&nbsp;&nbsp;author = {Deng, Yu and Yang, Jiaolong and Xin, Tong},<br/>
&nbsp;&nbsp;title = {Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {10286-10296},<br/>
&nbsp;&nbsp;year = {2020}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr20_disenfacegan.png" width="140"/><a name="disenfacegan" id="disenfacegan"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://yudeng.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Yu Deng</a><sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://www.microsoft.com/en-us/research/people/doch/" target="_blank" style="text-decoration:underline;color:#000;">Dong Chen</a>, <a href="https://www.microsoft.com/en-us/research/people/fangwen/" target="_blank" style="text-decoration:underline;color:#000;">Fang Wen</a>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning</span><br/>
<span class="jellon_d_i_c_refer">The 36th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2020)</b>, Seattle, USA</span> (<b><span style="color:#e60000">Oral</span></b>)<br/>
[<a href="#" onclick="show('21','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('21','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2004.11660.pdf">PDF</a>] [<a target="_blank" href="https://github.com/microsoft/DiscoFaceGAN">Code</a>] [<a target="_blank" href="https://arxiv.org/pdf/2004.11660.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2004.11660">arXiv</a>]<br/>
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="21_abstract">
We propose DiscoFaceGAN, an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity, expression, pose, and illumination. We embed 3D priors into adversarial learning and train the network to imitate the image formation of an analytic 3D face deformation and rendering process. To deal with the generation freedom induced by the domain gap between real and rendered faces, we further introduce contrastive learning to promote disentanglement by comparing pairs of generated images. Experiments show that through our imitative-contrastive learning, the factor variations are very well disentangled and the properties of a generated face can be precisely controlled. We also analyze the learned latent space and present several meaningful properties supporting factor disentanglement. Our method can also be used to embed real images into the disentangled latent space. We hope our method could provide new understandings of the relationship between physical properties and deep image synthesis.
</div>
<div id="21_bibtex">
@inproceedings{deng2020disentangled,<br/>
&nbsp;&nbsp;author = {Deng, Yu and Yang, Jiaolong and Chen, Dong and Wen, Fang and Xin, Tong},<br/>
&nbsp;&nbsp;title = {Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {5154-5163},<br/>
&nbsp;&nbsp;year = {2020}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr20_deep3dportrait.png" width="125"/>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;"><a name="deep3dportrait" id="deep3dportrait"></a>
Sicheng Xu<sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://www.microsoft.com/en-us/research/people/doch/" target="_blank" style="text-decoration:underline;color:#000;">Dong Chen</a>, <a href="https://www.microsoft.com/en-us/research/people/fangwen/" target="_blank" style="text-decoration:underline;color:#000;">Fang Wen</a>, <a href="https://yudeng.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Yu Deng</a>, <a href="http://iitlab.bit.edu.cn/mcislab/~jiayunde/" target="_blank" style="text-decoration:underline;color:#000;">Yunde Jia</a>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">Deep 3D Portrait from a Single Image</span><br/>
<span class="jellon_d_i_c_refer">The 36th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2020)</b>, Seattle, USA</span><br/>
[<a href="#" onclick="show('20','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('20','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2004.11598.pdf">PDF</a>] [<a target="_blank" href="https://github.com/sicxu/Deep3dPortrait">Code</a>] [<a target="_blank" href="https://arxiv.org/pdf/2004.11598.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2004.11598">arXiv</a>]<br/>
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="20_abstract">
In this paper, we present a learning-based approach for recovering the 3D geometry of human head from a single portrait image. Our method is learned in an unsupervised manner without any ground-truth 3D data. We represent the head geometry with a parametric 3D face model together with a depth map for other head regions including hair and ear. A two-step geometry learning scheme is proposed to learn 3D head reconstruction from in-the-wild face images, where we first learn face shape on single images using self-reconstruction and then learn hair and ear geometry using pairs of images in a stereo-matching fashion. The second step is based on the output of the first to not only improve the accuracy but also ensure the consistency of overall head geometry. We evaluate the accuracy of our method both in 3D and with pose manipulation tasks on 2D images. We alter pose based on the recovered geometry and apply a refinement network trained with adversarial learning to ameliorate the reprojected images and translate them to the real image domain. Extensive evaluations and comparison with previous methods show that our new method can produce high-fidelity 3D head geometry and head pose manipulation results.
</div>
<div id="20_bibtex">
@inproceedings{xu2020deep,<br/>
&nbsp;&nbsp;author = {Xu, Sicheng and Yang, Jiaolong and Chen, Dong and Wen, Fang and Deng, Yu and Jia, Yunde and Xin, Tong},<br/>
&nbsp;&nbsp;title = {Deep 3D Portrait from a Single Image},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {7710-7720},<br/>
&nbsp;&nbsp;year = {2020}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr20_lowlightdenosing.png" width="140"/><a name="lowlightdenosing" id="lowlightdenosing"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://kxwei.net/" target="_blank" style="text-decoration:underline;color:#000;">Kaixuan Wei</a>, <a href="https://ying-fu.github.io/" target="_blank" style="text-decoration:underline;color:#000;"> Ying Fu</a>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://iitlab.bit.edu.cn/gvlab/info/teaminfo/HuangHua/home_eng.html" target="_blank" style="text-decoration:underline;color:#000;">Hua Huang</a><br/>
<span class="jellon_d_i_c_name">A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising</span><br/>
<span class="jellon_d_i_c_refer">The 36th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2020)</b>, Seattle, USA</span> (<b><span style="color:#e60000">Oral</span></b>)<br/>
[<a href="#" onclick="show('19','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('19','bibtex');return false;">BibTex</a>] [<a target="_blank" href="https://arxiv.org/pdf/2003.12751.pdf">PDF</a>] [<a target="_blank" href="https://github.com/Vandermode/NoiseModel">Code</a>] [<a href="#" onclick="alert('Available soon');return false;">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/2003.12751">arXiv</a>]<br/>
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="19_abstract">
Lacking rich and realistic data, learned single image denoising algorithms generalize poorly to real raw images that do not resemble the data used for training. Although the problem can be alleviated by the heteroscedastic Gaussian model for noise synthesis, the noise sources caused by digital camera electronics are still largely overlooked, despite their significant effect on raw measurement, especially under extremely low-light condition. To address this issue, we present a highly accurate noise formation model based on the characteristics of CMOS photosensors, thereby enabling us to synthesize realistic samples that better match the physics of image formation process. Given the proposed noise model, we additionally propose a method to calibrate the noise parameters for available modern digital cameras, which is simple and reproducible for any new device. We systematically study the generalizability of a neural network trained with existing schemes, by introducing a new low-light denoising dataset that covers many modern digital cameras from diverse brands. Extensive empirical results collectively show that by utilizing our proposed noise formation model, a network can reach the capability as if it had been trained with rich real data, which demonstrates the effectiveness of our noise formation model.
</div>
<div id="19_bibtex">
@inproceedings{wei2020physics,<br/>
&nbsp;&nbsp;author = {Wei, Kaixuan and Fu, Ying and Yang, Jiaolong and Huang, Hua},<br/>
&nbsp;&nbsp;title = {A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {2758-2767},<br/>
&nbsp;&nbsp;year = {2020}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="iccv19_videofacedeblur.png" width="140"/><a name="videofacedeblur" id="videofacedeblur"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<a href="https://sites.google.com/site/renwenqi888/" target="_blank" style="text-decoration:underline;color:#000;">Wenqi Ren</a>*, <span class="jellon_d_i_c_emphasize">Jiaolong Yang*</span>, Senyou Deng, <a href="http://www.davidwipf.com/" target="_blank" style="text-decoration:underline;color:#000;">David Wipf</a>, <a href="https://xiaochun-cas.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Xiaochun Cao</a>, <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">Face Video Deblurring using 3D Facial Priors</span><br/>
<span class="jellon_d_i_c_refer">The 17th International Conference on Computer Vision <b>(ICCV2019)</b>, Seoul, Korea</span> (<b><span style="color:#e60000">Oral</span></b>)<br/>
[<a href="#" onclick="show('18','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('18','bibtex');return false;">BibTex</a>] [<a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Ren_Face_Video_Deblurring_Using_3D_Facial_Priors_ICCV_2019_paper.pdf">PDF</a>] [<a target="_blank" href="https://github.com/rwenqi/3Dfacedeblurring">Code</a>] [<a target="_blank" href="iccv19_videofacedeblur_supmat.pdf">Suppl. Material</a>] [<a href="#">arXiv</a>]<br/> 
(*: Equal contributions)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="18_abstract">
Existing face deblurring methods only consider single frames and do not account for facial structure and identity information. These methods struggle to deblur face videos that exhibit significant pose variations and misalignment. In this paper we propose a novel face video deblurring network capitalizing on 3D facial priors. The model consists of two main branches: i) a face video deblurring sub-network based on the encoder-decoder architecture, and ii) a 3D face rendering branch for predicting 3D priors of salient facial structures and identity knowledge. These structures encourage the deblurring branch to generate sharp faces with detailed structures. Our method not only uses low-level information (i.e., intensity similarity), but also middle-level information (i.e., 3D facial structure) and high-level knowledge (i.e., identity content) to further explore spatial constraints of facial components from blurry face frames. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods. 
</div>
<div id="18_bibtex">
@inproceedings{ren2019face,<br/>
&nbsp;&nbsp;author = {Ren, Wenqi and Yang, Jiaolong and Deng, Senyou and Wipf, David and Cao, Xiaochun and Xin, Tong},<br/>
&nbsp;&nbsp;title = {Face Video Deblurring using a 3D Facial Prior},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},<br/>
&nbsp;&nbsp;pages = {9388-9397},<br/>
&nbsp;&nbsp;year = {2019}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr19_reflectionremoval.png" width="140"/><a name="ERRNet" id="ERRNet"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://kxwei.net/" target="_blank" style="text-decoration:underline;color:#000;">Kaixuan Wei</a>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://ying-fu.github.io/" target="_blank" style="text-decoration:underline;color:#000;"> Ying Fu</a>,  <a href="http://www.davidwipf.com/" target="_blank" style="text-decoration:underline;color:#000;">David Wipf</a>, <a href="http://iitlab.bit.edu.cn/gvlab/info/teaminfo/HuangHua/home_eng.html" target="_blank" style="text-decoration:underline;color:#000;">Hua Huang</a><br/>
<span class="jellon_d_i_c_name">Single Image Reflection Removal Exploiting Misaligned Training Data and Network Enhancements</span><br/>
<span class="jellon_d_i_c_refer">The 35th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2019)</b>, Long Beach, USA</span><br/>
[<a href="#" onclick="show('17','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('17','bibtex');return false;">BibTex</a>] [<a target="_blank" href="cvpr19_reflectionremoval.pdf">PDF</a>] [<a target="_blank" href="https://github.com/Vandermode/ERRNet">Code</a>] [<a target="_blank" href="cvpr19_reflectionremoval_supmat.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/1904.00637">arXiv</a>]<br/> 
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="17_abstract">
Removing undesirable reflections from a single image captured through a glass window is of practical importance to visual computing systems. Although state-of-the-art methods can obtain decent results in certain situations, performance declines significantly when tackling more general real-world cases. These failures stem from the intrinsic difficulty of single image reflection removal--the fundamental ill-posedness of the problem, and the insufficiency of densely-labeled training data needed for resolving this ambiguity within learning-based neural network pipelines. In this paper, we address these issues by exploiting targeted network enhancements and the novel use of misaligned data. For the former, we augment a baseline network architecture by embedding context encoding modules that are capable of leveraging high-level contextual clues to reduce indeterminacy within areas containing strong reflections. For the latter, we introduce an alignment-invariant loss function that facilitates exploiting misaligned real-world training data that is much easier to collect. Experimental results collectively show that our method outperforms the state-of-the-art with aligned data, and that significant improvements are possible when using additional misaligned data.
</div>
<div id="17_bibtex">
@inproceedings{wei2019single,<br/>
&nbsp;&nbsp;author = {Wei, Kaixuan and Yang, Jiaolong and Fu, Ying and Wipf, David and Huang, Hua},<br/>
&nbsp;&nbsp;title = {Single Image Reflection Removal Exploiting Misaligned Training Data and Network Enhancements},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {8178-8187},<br/>
&nbsp;&nbsp;year = {2019}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvprw19_3dface.png" width="140"/><a name="face3drecon" id="face3drecon"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<a href="https://yudeng.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Yu Deng</a><sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, Sicheng Xu, <a href="https://www.microsoft.com/en-us/research/people/doch/" target="_blank" style="text-decoration:underline;color:#000;">Dong Chen</a>, <a href="http://iitlab.bit.edu.cn/mcislab/~jiayunde/" target="_blank" style="text-decoration:underline;color:#000;">Yunde Jia</a>, and <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</span><br/>
<span class="jellon_d_i_c_refer">IEEE Computer Vision and Pattern Recognition Workshop on AMFG<b> (CVPRW2019)</b>, Long Beach, USA</span> (<span style="color:#e60000"><b><a href="Deep3DFace_CVPRW19-AMFG_Best_Paper_Award_Certification.jpg" target="_blank" style="color:#e60000;text-decoration:underline;">Best Paper Award</a></b>. Check out our code!</span>)<br/>
[<a href="#" onclick="show('16','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('16','bibtex');return false;">BibTex</a>] [<a target="_blank" href="cvprw19_3dface.pdf">PDF</a>] [<a target="_blank" href="https://github.com/sicxu/Deep3DFaceRecon_pytorch">Code</a>] [<a target="_blank" href="cvprw19_3dface_supmat.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/1903.08527">arXiv</a>]<br/> 
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="16_abstract">
Recently, deep learning based 3D face reconstruction methods have shown promising results in both quality and efficiency. However, training deep neural networks typically requires a large volume of data, whereas face images with ground-truth 3D face shapes are scarce. In this paper, we propose a novel deep 3D face reconstruction approach that 1) leverages a robust, hybrid loss function for weakly-supervised learning which takes into account both
low-level and perception-level information for supervision, and 2) performs multi-image face reconstruction by exploiting complementary information from different images for shape aggregation. Our method is fast, accurate, and robust to occlusion and large pose. We provide comprehensive experiments on MICC Florence and Facewarehouse datasets, systematically comparing our method with fifteen recent methods and demonstrating its state-of-the-art performance.
</div>
<div id="16_bibtex">
@inproceedings{deng2019accurate,<br/>
&nbsp;&nbsp;author = {Deng, Yu and Yang, Jiaolong and Xu, Sicheng and Chen, Dong and Jia, Yunde and Tong, Xin},<br/>
&nbsp;&nbsp;title = {Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of IEEE Computer Vision and Pattern Recognition Workshop on Analysis and Modeling of Faces and Gestures},<br/>
<!--&nbsp;&nbsp;pages = {111-126},<br/>-->
&nbsp;&nbsp;year = {2019}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="aaai19_singleview3d.png" width="140"/><a name="object3drecon" id="object3drecon"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://hanqingwangai.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Hanqing Wang</a><sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://iitlab.bit.edu.cn/mcislab/~liangwei/" target="_blank" style="text-decoration:underline;color:#000;">Wei Liang</a> and <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">Deep Single-View 3D Object Reconstruction with Visual Hull Embedding</span><br/>
<span class="jellon_d_i_c_refer">The 33rd AAAI Conference on Artificial Intelligence <b>(AAAI2019)</b>, Honolulu, USA</span> (<b><span style="color:#e60000">Oral</span></b>)<br/>
[<a href="#" onclick="show('15','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('15','bibtex');return false;">BibTex</a>] [<a target="_blank" href="aaai19_singleview3d.pdf">PDF</a>] [<a target="_blank" href="https://github.com/qweas120/PSVH-3d-reconstruction">Code</a>] [<a target="_blank" href="aaai19_singleview3d_supmat.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/1809.03451">arXiv</a>]<br/> 
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="15_abstract">
3D object reconstruction is a fundamental task of many robotics and AI problems. With the aid of deep convolutional neural networks (CNNs), 3D object reconstruction has witnessed a significant progress in recent years. However, possibly due to the prohibitively high dimension of the 3D object space, the results from deep CNNs are often prone to missing some shape details. In this paper, we present an approach which aims to preserve more shape details and improve the reconstruction quality. The key idea of our method is to leverage object mask and pose estimation from CNNs to assist the 3D shape learning by constructing a probabilistic single-view visual hull inside of the network. Our method works by first predicting a coarse shape as well as the object pose and silhouette using CNNs, followed by a novel 3D refinement CNN which refines the coarse shapes using the constructed probabilistic visual hulls. Experiment on both synthetic data and real images show that embedding a single-view visual hull for shape refinement can significantly improve the reconstruction quality by recovering more shapes details and improving shape consistency with the input image.
</div>
<div id="15_bibtex">
@inproceedings{wang2019deep,<br/>
&nbsp;&nbsp;author = {Wang, Hanqing and Yang, Jiaolong and Liang, Wei and Tong, Xin},<br/>
&nbsp;&nbsp;title = {Deep Single-View 3D Object Reconstruction with Visual Hull Embedding},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},<br/>
<!--&nbsp;&nbsp;pages = {111-126},<br/>-->
&nbsp;&nbsp;year = {2019}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="tog18_thumbnail.png" width="140"/><a name="imagesmoothing" id="imagesmoothing"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<a href="https://fqnchina.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Qingnan Fan</a><sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>,  <a href="http://www.davidwipf.com/" target="_blank" style="text-decoration:underline;color:#000;">David Wipf</a>, <a href="http://www.cs.sdu.edu.cn/~baoquan" target="_blank" style="text-decoration:underline;color:#000;">Baoquan Chen</a> and <a href="https://www.microsoft.com/en-us/research/people/xtong/" target="_blank" style="text-decoration:underline;color:#000;">Xin Tong</a><br/>
<span class="jellon_d_i_c_name">Image Smoothing via Unsupervised Learning</span><br/>
<span class="jellon_d_i_c_refer">ACM Transactions on Graphics <b>(TOG)</b>, 2018 (Proc. <b>SIGGRAPH Asia 2018</b>)</span> <br/>
[<a href="#" onclick="show('14','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('14','bibtex');return false;">BibTex</a>] [<a target="_blank" href="tog18_imagesmoothing.pdf">PDF</a>] [<a target="_blank" href="https://github.com/fqnchina/ImageSmoothing">Code</a>] [<a target="_blank" href="https://drive.google.com/open?id=1un1KGSiVl3mi_d0Xq6tgZN2Ffrp85MPh">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/1811.02804">arXiv</a>]<br/> 
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="14_abstract">
Image smoothing represents a fundamental component of many disparate computer vision and graphics applications. In this paper, we present a unified unsupervised (label-free) learning framework that facilitates generating flexible and high-quality smoothing effects by directly learning from data using deep convolutional neural networks (CNNs). The heart of the design is the training signal as a novel energy function that includes an edge-preserving regularizer which helps maintain important yet potentially vulnerable image structures, and a spatially-adaptive Lp flattening criterion which imposes different forms of regularization onto different image regions for better smoothing quality. We implement a diverse set of image smoothing solutions employing the unified framework targeting various applications such as, image abstraction, pencil sketching, detail enhancement, texture removal and content-aware image manipulation, and obtain results comparable with or better than previous methods. Moreover, our method is extremely fast with a modern GPU (e.g, 200 fps for 1280×720 images).</div>
<div id="14_bibtex">
@article{fan2018image,<br/>
&nbsp;&nbsp;author = {Fan, Qingnan and Yang, Jiaolong and Wipf, David and Chen, Baoquan and Tong, Xin},<br/>
&nbsp;&nbsp;title = {Image Smoothing via Unsupervised Learning},<br/>
&nbsp;&nbsp;booktitle = {ACM Transactions on Graphics},<br/>
&nbsp;&nbsp;volume = {37},
&nbsp;&nbsp;number = {6},
&nbsp;&nbsp;pages = {1--14},
&nbsp;&nbsp;year = {2018}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="eccv18_thumbnail.png" width="140"/><a name="dnnquantization", id="dnnquantization"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
Dongqing Zhang*, <span class="jellon_d_i_c_emphasize">Jiaolong Yang*</span>, Dongqiangzi Ye* and <a href="http://www.ganghua.org/" target="_blank" style="text-decoration:underline;color:#000;">Gang Hua</a><br/>
<span class="jellon_d_i_c_name">LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks</span><br/>
<span class="jellon_d_i_c_refer">The 15th European Conference on Computer Vision <b>(ECCV2018)</b>, Munich, Germany</span><br/>
[<a href="#" onclick="show('13','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('13','bibtex');return false;">BibTex</a>] [<a target="_blank" href="eccv18_dnnquantization.pdf">PDF</a>] [<a target="_blank" href="https://github.com/Microsoft/LQ-Nets">Code</a>] [<a target="_blank" href="eccv18_dnnquantization_suppmat.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/1807.10029">arXiv</a>]<br/> 
(*: Equal contributions)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="13_abstract">
Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets
</div>
<div id="13_bibtex">
@inproceedings{zhang2018optimized,<br/>
&nbsp;&nbsp;author = {Zhang, Dongqiang and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},<br/>
&nbsp;&nbsp;title = {LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},<br/>
<!--&nbsp;&nbsp;pages = {111-126},<br/>-->
&nbsp;&nbsp;year = {2018}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr18_thumbnail.png" width="140"/><a name="deepintrinsicimage" id="deepintrinsicimage"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<a href="https://fqnchina.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Qingnan Fan</a><sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>,  <a href="http://www.ganghua.org/" target="_blank" style="text-decoration:underline;color:#000;">Gang Hua</a>, <a href="http://www.cs.sdu.edu.cn/~baoquan" target="_blank" style="text-decoration:underline;color:#000;">Baoquan Chen</a> and <a href="http://www.davidwipf.com/" target="_blank" style="text-decoration:underline;color:#000;">David Wipf</a><br/>
<span class="jellon_d_i_c_name">Revisiting Deep Intrinsic Image Decompositions</span><br/>
<span class="jellon_d_i_c_refer">The 34th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2018)</b>, Salt Lake City, USA</span> (<b><span style="color:#e60000">Oral</span></b>)<br/>
[<a href="#" onclick="show('12','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('12','bibtex');return false;">BibTex</a>] [<a target="_blank" href="cvpr18_deepintrinsicimage.pdf">PDF</a>] [<a target="_blank" href="https://github.com/fqnchina/IntrinsicImage">Code</a>] [<a target="_blank" href="cvpr18_deepintrinsicimage_suppmat.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/1701.02965">arXiv</a>]<br/> 
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="12_abstract">
While invaluable for many computer vision applications, decomposing a natural image into intrinsic reflectance and shading layers represents a challenging, underdetermined inverse problem. As opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions, deep learning-based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data.  The downside is that current data sources are quite limited, and broadly speaking fall into one of two categories: either dense fully-labeled images in synthetic/narrow settings, or weakly-labeled data from relatively diverse natural scenes.  In contrast to many previous learning-based approaches, which are often tailored to the structure of a particular dataset (and may not work well on others), we adopt core network structures that universally reflect loose prior knowledge regarding the intrinsic image formation process and can be largely shared across datasets.  We then apply flexibly supervised loss layers that are customized for each source of ground truth labels.  The resulting deep architecture achieves state-of-the-art results on all of the major intrinsic image benchmarks, and runs considerably faster than most at test time.
</div>
<div id="12_bibtex">
@inproceedings{fan2018revisiting,<br/>
&nbsp;&nbsp;author = {Fan, Qingnan and Yang, Jiaolong and Hua, Gang and Chen, Baoquan and Wipf, David},<br/>
&nbsp;&nbsp;title = {Revisiting Deep Intrinsic Image Decompositions},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 34th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {8944-8952},<br/>
&nbsp;&nbsp;year = {2018}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="iccv17_thumbnail.png" width="140"/><a name="dcnnreflectionsmoothing" id="dcnnreflectionsmoothing"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<a href="https://fqnchina.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Qingnan Fan</a><sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>,  <a href="http://www.ganghua.org/" target="_blank" style="text-decoration:underline;color:#000;">Gang Hua</a>, <a href="http://www.cs.sdu.edu.cn/~baoquan" target="_blank" style="text-decoration:underline;color:#000;">Baoquan Chen</a> and <a href="http://www.davidwipf.com/" target="_blank" style="text-decoration:underline;color:#000;">David Wipf</a><br/>
<span class="jellon_d_i_c_name">A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing</span><br/>
<span class="jellon_d_i_c_refer">The 16th International Conference on Computer Vision <b>(ICCV2017)</b>, Venice, Italy</span><br/>
[<a href="#" onclick="show('11','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('11','bibtex');return false;">BibTex</a>] [<a target="_blank" href="iccv17_dcnnreflectionsmoothing.pdf">PDF</a>] [<a target="_blank" href="https://github.com/fqnchina/CEILNet">Code</a>] [<a target="_blank" href="iccv17_dcnnreflectionsmoothing_supmat.pdf">Suppl. Material</a>] [<a target="_blank" href="https://arxiv.org/abs/1708.03474">arXiv</a>]<br/> 
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="11_abstract">
This paper proposes a deep neural network structure that exploits edge information in addressing representative low-level vision tasks such as layer separation and image filtering. Unlike most other deep learning strategies applied in this context, our approach tackles these challenging problems by estimating edges and reconstructing images using only cascaded convolutional layers arranged such that no handcrafted or application-specific image-processing components are required.  We apply the resulting transferrable pipeline to two different problem domains that are both sensitive to edges, namely, single image reflection removal and image smoothing. For the former, using a mild reflection smoothness assumption and a novel synthetic data generation method that acts as a type of weak supervision, our network is able to solve much more difficult reflection cases that cannot be handled by previous methods. For the latter, we also exceed the state-of-the-art quantitative and qualitative results by wide margins.  In all cases, the proposed framework is simple, fast, and easy to transfer across disparate domains. </div>
<div id="11_bibtex">
@inproceedings{fan2017generic,<br/>
&nbsp;&nbsp;author = {Fan, Qingnan and Yang, Jiaolong and Hua, Gang and Chen, Baoquan and Wipf, David},<br/>
&nbsp;&nbsp;title = {A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 16th International Conference on Computer Vision (ICCV)},<br/>
&nbsp;&nbsp;pages = {3238-3247},<br/>
&nbsp;&nbsp;year = {2017}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvprw17_thumbnail.png" width="140"/>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
Chen Zhou<sup>+</sup>, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="https://www.microsoft.com/en-us/research/people/chunzhao/" target="_blank" style="text-decoration:underline;color:#000;">Chunshui Zhao</a> and <a href="http://www.ganghua.org/" target="_blank" style="text-decoration:underline;color:#000;">Gang Hua</a><br/>
<span class="jellon_d_i_c_name">Fast, Accurate Thin-Structure Obstacle Detection for Autonomous Mobile Robots</span><br/>
<span class="jellon_d_i_c_refer">IEEE Computer Vision and Pattern Recognition Workshop on Embedded Vision <b>(CVPRW2017)</b>, Honolulu, USA</span><br/>
[<a href="#" onclick="show('10','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('10','bibtex');return false;">BibTex</a>] [<a target="_blank" href="cvprw17_thinobstacledetection.pdf">PDF</a>] [<a target="_blank" href="https://arxiv.org/abs/1708.04006">arXiv</a>]<br/>
(<sup>+</sup>: Intern at MSRA)
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="10_abstract">
Safety is paramount for mobile robotic platforms such as self-driving cars and unmanned aerial vehicles. This work is devoted to a task that is indispensable for safety yet was largely overlooked in the past -- detecting obstacles that are of very thin structures, such as wires, cables and tree branches. This is a challenging problem, as thin objects can be problematic for active sensors such as lidar and sonar and even for stereo cameras. In this work, we propose to use video sequences for thin obstacle detection. We represent obstacles with edges in the video frames, and reconstruct them in 3D using efficient edge-based visual odometry techniques. We provide both a monocular camera solution and a stereo camera solution. The former incorporates Inertial Measurement Unit (IMU) data to solve scale ambiguity, while the latter enjoys a novel, purely vision-based solution. Experiments demonstrated that the proposed methods are fast and able to detect thin obstacles robustly and accurately under various conditions. </div>
<div id="10_bibtex">
@inproceedings{yang2017neural,<br/>
&nbsp;&nbsp;author = {Zhou, Chen and Yang, Jiaolong and Zhao, Chunshui and Hua, Gang},<br/>
&nbsp;&nbsp;title = {Fast, Accurate Thin-Structure Obstacle Detection for Autonomous Mobile Robots},<br/>
&nbsp;&nbsp;booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},<br/>
&nbsp;&nbsp;pages = {1-10},<br/>
&nbsp;&nbsp;year = {2017}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr17_thumbnail.png" width="140"/><a name="videoface" id="videoface"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://renpr.org/" target="_blank" style="text-decoration:underline;color:#000;">Peiran Ren</a>, Dongqing Zhang, <a href="https://www.microsoft.com/en-us/research/people/doch/" target="_blank" style="text-decoration:underline;color:#000;">Dong Chen</a>, <a href="https://www.microsoft.com/en-us/research/people/fangwen/" target="_blank" style="text-decoration:underline;color:#000;">Fang Wen</a>, <a href="http://users.cecs.anu.edu.au/~hongdong/" target="_blank" style="text-decoration:underline;color:#000;">Hongdong Li</a> and <a href="http://www.ganghua.org/" target="_blank" style="text-decoration:underline;color:#000;">Gang Hua</a><br/>
<span class="jellon_d_i_c_name">Neural Aggregation Network for Video Face Recognition</span><br/>
<span class="jellon_d_i_c_refer">The 33th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2017)</b>, Honolulu, USA</span></span><br/>
[<a href="#" onclick="show('9','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('9','bibtex');return false;">BibTex</a>] [<a target="_blank" href="cvpr17_videoface.pdf">PDF</a>] [<a target="_blank" href="http://arxiv.org/abs/1603.05474">arXiv</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="9_abstract">
We present a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with a variable number of face images as its input, and produces a compact and fixed-dimension feature representation. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN), which maps each face image into a feature vector. The aggregation module consists of two attention blocks driven by a memory storing all the extracted features. It adaptively aggregates the features to form a single feature inside the convex hull spanned by them. Due to the attention mechanism, the aggregation is invariant to the image order. We found that NAN learns to advocate high-quality face images while repelling low-quality ones such as blurred, occluded and improperly exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face recognition benchmarks show that it consistently outperforms standard aggregation methods and achieves state-of-the-art accuracies. </div>
<div id="9_bibtex">
@inproceedings{yang2017neural,<br/>
&nbsp;&nbsp;author = {Yang, Jiaolong and Ren, Peiran and Zhang, Dongqing and Chen, Dong and Wen, Fang and Li, Hongdong and Hua, Gang},<br/>
&nbsp;&nbsp;title = {Neural Aggregation Network for Video Face Recognition},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 32th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {4362-4371},<br/>
&nbsp;&nbsp;year = {2017}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr16_thumbnail.jpg" width="140"/><a name="robustopticalflow" id="robustopticalflow"></a>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://users.cecs.anu.edu.au/~hongdong/" target="_blank" style="text-decoration:underline;color:#000;">Hongdong Li</a>, <a href="http://users.cecs.anu.edu.au/~yuchao/" target="_blank" style="text-decoration:underline;color:#000;">Yuchao Dai</a> and <a href="http://tanrobby.github.io/" target="_blank" style="text-decoration:underline;color:#000;">Robby T. Tan</a><br/>
<span class="jellon_d_i_c_name">Robust Optical Flow Estimation of Double-Layer Images under Transparency or Reflection</span><br/>
<span class="jellon_d_i_c_refer">The 32th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2016)</b>, Las Vegas, USA</span></span><br/>
[<a href="#" onclick="show('8','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('8','bibtex');return false;">BibTex</a>] [<a target="_blank" href="cvpr16_robustopticalflow.pdf">PDF</a>] [<a target="_blank" href="cvpr16_robustopticalflow_code.zip">Code</a>] [<a target="_blank" href="cvpr16_robustopticalflow_supmat.pdf">Suppl. Material</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="8_abstract">
This paper deals with a challenging, frequently encountered, yet not properly investigated problem in two-frame optical flow estimation. That is, the input frames are compounds of two imaging layers - one desired background layer of the scene, and one distracting, possibly moving layer due to transparency or reflection. In this situation, the conventional brightness constancy constraint - the cornerstone of most existing optical flow methods - will no longer be valid. In this paper, we propose a robust solution to this problem. The proposed method performs both optical flow estimation, and image layer separation. It exploits a generalized double-layer brightness consistency constraint connecting these two tasks, and utilizes the priors for both of them. Experiments on both synthetic data and real images have confirmed the efficacy of the proposed method. To the best of our knowledge, this is the first attempt towards handling generic optical flow fields of two-frame images containing transparency or reflection.</div>
<div id="8_bibtex">
@inproceedings{yang2016robust,<br/>
&nbsp;&nbsp;author = {Yang, Jiaolong and Li, Hongdong and Dai, Yuchao and Tan, Robby T.},<br/>
&nbsp;&nbsp;title = {Robust Optical Flow Estimation of Double-Layer Images under Transparency or Reflection},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 32th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {1410-1419},<br/>
&nbsp;&nbsp;year = {2016}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="tpami16_thumbnail.png" width="140"/><a name="goicpjournal" id="goicpjournal"></a>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://users.cecs.anu.edu.au/~hongdong/" target="_blank" style="text-decoration:underline;color:#000;">Hongdong Li</a>, Dylan Campbell and <a href="http://iitlab.bit.edu.cn/mcislab/~jiayunde/index_en.htm" target="_blank" style="text-decoration:underline;color:#000;">Yunde Jia</a><br/>
<span class="jellon_d_i_c_name">Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration</span><br/>
<span class="jellon_d_i_c_refer">IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2016</span><br/>
[<a href="#" onclick="show('7','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('7','bibtex');return false;">BibTex</a>] [<a target="_blank" href="tpami16_go-icp_preprint.pdf">PDF</a>] [<a target="_blank" href="./go-icp#code">Code</a>] [<a target="_blank" href="./go-icp">Webpage</a>] [<a target="_blank" href="tpami16_go-icp_supmat.pdf">Suppl. Material</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="7_abstract">
The Iterative Closest Point (ICP) algorithm is one of the most widely used methods for point-set registration. However, being based on local iterative optimization, ICP is known to be susceptible to local minima. Its performance critically relies on the quality of the initialization and only local optimality is guaranteed. This paper presents the first globally optimal algorithm, named Go-ICP, for Euclidean (rigid) registration of two 3D point-sets under the L2 error metric defined in ICP. The Go-ICP method is based on a branch-and-bound (BnB) scheme that searches the entire 3D motion space SE(3). By exploiting the special structure of SE(3) geometry, we derive novel upper and lower bounds for the registration error function. Local ICP is integrated into the BnB scheme, which speeds up the new method while guaranteeing global optimality. We also discuss extensions, addressing the issue of outlier robustness. The evaluation demonstrates that the proposed method is able to produce reliable registration results regardless of the initialization. Go-ICP can be applied in scenarios where an optimal solution is desirable or where a good initialization is not always available.</div>
<div id="7_bibtex">
@article{yang2016goicp,<br/>
&nbsp;&nbsp;author = {Yang, Jiaolong and Li, Hongdong and Campbell, Dylan and Jia, Yunde},<br/>
&nbsp;&nbsp;title = {Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration},<br/>
&nbsp;&nbsp;journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)},<br/>
&nbsp;&nbsp;volume = {38},<br/>
&nbsp;&nbsp;number = {11},<br/>
&nbsp;&nbsp;pages = {2241--2254},<br/>
&nbsp;&nbsp;year = {2016}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="cvpr15_thumbnail.png" width="140"/>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<span class="jellon_d_i_c_emphasize">Jiaolong Yang</span> and <a href="http://users.cecs.anu.edu.au/~hongdong/" target="_blank" style="text-decoration:underline;color:#000;">Hongdong Li</a> <br/>
<span class="jellon_d_i_c_name">Dense, Accurate Optical Flow Estimation with Piecewise Parametric Model</span><br/>
<span class="jellon_d_i_c_refer">The 31th IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR2015)</b>, Boston, USA</span><br/>
[<a href="#" onclick="show('6','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('6','bibtex');return false;">BibTex</a>] [<a target="_blank" href="cvpr15_opticalflow.pdf">PDF</a>] [<a href="#" onclick="alert('Available soon');return false;">Code</a>] [<a target="_blank" href="cvpr15_opticalflow_extabstract.pdf">Extended Abstract</a>] [<a target="_blank" href="cvpr15_opticalflow_supmat.pdf">Suppl. Material</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="6_abstract">
This paper proposes a simple method for estimating dense and accurate optical flow field. It revitalizes an early idea of piecewise parametric flow model. A key innovation is that we fit a flow field piecewise to a variety of parametric models, where the domain of each piece (i.e., each piece's shape, position and size) as well as the total number of pieces are determined adaptively, while at the same time maintaining a global inter-piece flow continuity constraint.  We achieve this by a multi-model fitting scheme via energy minimization. Our energy takes into account both the piecewise constant model assumption, and the flow field continuity constraint. The proposed method effectively handles both homogeneous regions and complex motion. Experiments on three public optical flow benchmarks (KITTI, MPI Sintel, and Middlebury) show the superiority of our method compared with the state of the art: it achieves top-tier performances on all the three benchmarks.
</div>
<div id="6_bibtex">
@inproceedings{yang2015dense,<br/>
&nbsp;&nbsp;author = {Yang, Jiaolong and Li, Hongdong},<br/>
&nbsp;&nbsp;title = {Dense, Accurate Optical Flow Estimation with Piecewise Parametric Model},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 31th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br/>
&nbsp;&nbsp;pages = {1019-1027},<br/>
&nbsp;&nbsp;year = {2015}<br/>
}
</div>
</td>
</tr>

<tr>
<td width="132px"><img src="eccv14_thumbnail.jpg" width="140"/>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://users.cecs.anu.edu.au/~hongdong/" target="_blank" style="text-decoration:underline;color:#000;">Hongdong Li</a> and <a href="http://iitlab.bit.edu.cn/mcislab/~jiayunde/index_en.htm" target="_blank" style="text-decoration:underline;color:#000;">Yunde Jia</a><br/>
<span class="jellon_d_i_c_name">Optimal Essential Matrix Estimation via Inlier-Set Maximization</span><br/>
<span class="jellon_d_i_c_refer">The 13th European Conference on Computer Vision <b>(ECCV2014)</b>, Zürich, Switzerland</span><br/>
<!--<span style="color:#800000;"><i>Received a Student Conference Grant</i></span><br/>-->
[<a href="#" onclick="show('5','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('5','bibtex');return false;">BibTex</a>] [<a target="_blank" href="eccv14_optimalematrix.pdf">PDF</a>] [<a target="_blank" href="eccv14_optimalematrix_code.zip">Code</a>] [<a target="_blank" href="https://drive.google.com/open?id=0B8ue2sp0bidyeHpXdXRpc0Q1bTQ">Data</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="5_abstract">
In this paper, we extend the globally optimal "rotation space search" method [11] to essential matrix estimation in the presence of feature mismatches or outliers. The problem is formulated as inlier-set cardinality maximization, and solved via branch-and-bound global optimization which searches the entire essential manifold formed by all essential matrices.  Our main contributions include an explicit, geometrically meaningful essential manifold parametrization using a 5D direct product space of a solid 2D disk and a solid 3D ball, as well as efficient closed-form bounding functions. Experiments on both synthetic data and real images have confirmed the efficacy of our method. The method is mostly suitable for applications where robustness and accuracy are paramount. It can also be used as a benchmark for method evaluation.
</div>
<div id="5_bibtex">
@inproceedings{yang2014optimal,<br/>
&nbsp;&nbsp;author = {Yang, Jiaolong and Li, Hongdong and Jia, Yunde},<br/>
&nbsp;&nbsp;title = {Optimal Essential Matrix Estimation via Inlier-Set Maximization},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 14th European Conference on Computer Vision (ECCV)},<br/>
&nbsp;&nbsp;pages = {111-126},<br/>
&nbsp;&nbsp;year = {2014}<br/>
}
</div>
</td>
</tr>

<tr>
<td><img src="iccv13_thumbnail.jpg" width="140"/>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://users.cecs.anu.edu.au/~hongdong/" target="_blank" style="text-decoration:underline;color:#000;">Hongdong Li</a> and <a href="http://iitlab.bit.edu.cn/mcislab/~jiayunde/index_en.htm" target="_blank" style="text-decoration:underline;color:#000;">Yunde Jia</a><br/>
<span class="jellon_d_i_c_name">Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</span><br/>
<span class="jellon_d_i_c_refer">The 14th International Conference on Computer Vision <b>(ICCV2013)</b>, Sydney, Australia</span><br/>
[<a href="#" onclick="show('4','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('4','bibtex');return false;">BibTex</a>] [<a target="_blank" href="iccv13_go-icp.pdf">PDF</a>] [<a target="_blank" href="./go-icp#code">Code</a>] [<a target="_blank" href="./go-icp">Webpage</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="4_abstract">
Registration is a fundamental task in computer vision. The Iterative Closest Point (ICP) algorithm is one of the widely-used methods for solving the registration problem. Based on local iteration, ICP is however well-known to suffer from local minima. Its performance critically relies on the quality of initialization, and only local optimality is guaranteed. This paper provides the very first globally optimal solution to Euclidean registration of two 3D pointsets or two 3D surfaces under the L2 error. Our method is built upon ICP, but combines it with a branch-and-bound (BnB) scheme which searches the 3D motion space SE(3) efficiently. By exploiting the special structure of the underlying geometry, we derive novel upper and lower bounds for the ICP error function. The integration of local ICP and global BnB enables the new method to run efficiently in practice, and its optimality is exactly guaranteed. We also discuss extensions, addressing the issue of outlier robustness.
</div>
<div id="4_bibtex">
@inproceedings{yang2013goicp,<br/>
&nbsp;&nbsp;author = {Yang, Jiaolong and Li, Hongdong and Jia, Yunde},<br/>
&nbsp;&nbsp;title = {Go-ICP: Solving 3D Registration Efficiently and Globally Optimally},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 14th International Conference on Computer Vision (ICCV)},<br/>
&nbsp;&nbsp;pages = {1457-1464},<br/>
&nbsp;&nbsp;year = {2013}<br/>
}
</div>
</td>
</tr>

<tr>
<td><img src="ismar13_thumbnail.jpg" width="140"/>
</td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:2px;">
<span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://users.cecs.anu.edu.au/~yuchao/" target="_blank" style="text-decoration:underline;color:#000;">Yuchao Dai</a>, <a href="http://users.cecs.anu.edu.au/~hongdong/" target="_blank" style="text-decoration:underline;color:#000;">Hongdong Li</a>, <a href="http://cs.anu.edu.au/~Henry.Gardner/" target="_blank" style="text-decoration:underline;color:#000;">Henry Gardner</a> and <a href="http://iitlab.bit.edu.cn/mcislab/~jiayunde/index_en.htm" target="_blank" style="text-decoration:underline;color:#000;">Yunde Jia</a><br/>
<span class="jellon_d_i_c_name">Single-shot Extrinsic Calibration of a Generically Configured RGB-D Camera Rig from Scene Constraints</span><br/>
<span class="jellon_d_i_c_refer">The 12th International Symposium on Mixed and Augmented Reality <b>(ISMAR2013)</b>, Adelaide, Australia</span> (<b><span style="color:#e60000">Oral</span></b>)<br/>
<!--<span style="color:#800000;"><i>Regular Paper with Oral Presentation</i></span><br/>-->
[<a href="#" onclick="show('3','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('3','bibtex');return false;">BibTex</a>] [<a target="_blank" href="ismar13_rgbdcalib.pdf">PDF</a>] [<a target="_blank" href="ismar13_rgbdcalib_slides.pdf">Slides</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#FFFFFF;padding-left:10px;padding-right:4px;">
<div id="3_abstract">
With the increasingly popular use of commodity RGB-D cameras for computer vision, robotics, mixed and augmented reality and other areas, it is of significant practical interest to calibrate the relative pose between a depth (D) camera and an RGB camera in these types of setups. In this paper, we propose a new single-shot, correspondence-free method to extrinsically calibrate a generically configured RGB-D camera rig. We formulate the extrinsic calibration problem as one of geometric 2D-3D registration which exploits scene constraints to achieve single-shot extrinsic calibration. Our method first reconstructs sparse point clouds from single view 2D image, which are then registered with dense point clouds from the depth camera. Finally, we directly optimize the warping quality by evaluating scene constraints in 3D point clouds. Our single-shot extrinsic calibration method does not require correspondences across multiple color images or across modality, achieving greater flexibility over existing methods. The scene constraints required by our method can be very simple and we demonstrate that a scene made up of three sheets of paper is sufficient to obtain reliable calibration and with lower geometric error than existing methods.
</div>
<div id="3_bibtex">
@inproceedings{yang2013single,<br/>
&nbsp;&nbsp;author = {Yang, Jiaolong and Dai, Yuchao and Li, Hongdong and Gardner, Henry and Jia, Yunde},<br/>
&nbsp;&nbsp;title = {Single-shot Extrinsic Calibration of a Generically Configured RGB-D Camera Rig from Scene Constraints},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 12th International Symposium on Mixed and Augmented Reality (ISMAR)},<br/>
&nbsp;&nbsp;pages = {181-188},<br/>
&nbsp;&nbsp;year = {2013}<br/>
}
</div>
</td>
</tr>

<tr>
<td><img src="icpr12_thumbnail.jpg" width="140"/>
</td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, <a href="http://iitlab.bit.edu.cn/mcislab/~liangwei/" target="_blank" style="text-decoration:underline;color:#000;">Wei Liang</a> and <a href="http://iitlab.bit.edu.cn/mcislab/~jiayunde/index_en.htm" target="_blank" style="text-decoration:underline;color:#000;">Yunde Jia</a><br/>
<span class="jellon_d_i_c_name">Face Pose Estimation with Combined 2D and 3D HOG Features</span><br/>
<span class="jellon_d_i_c_refer">The 21st International Conference on Pattern Recognition <b>(ICPR2012)</b>, Tsukuba, Japan</span><br/>
[<a href="#" onclick="show('2','abstract');return false;">Abstract</a>] [<a href="#" onclick="show('2','bibtex');return false;">BibTex</a>] [<a target="_blank" href="icpr12_facepose.pdf">PDF</a>]
</td>
</tr>
<tr style="width:100%;height:100%">
<td></td>
<td style="background:#F6F6F6;padding-left:10px;padding-right:4px;">
<div id="2_abstract">
In this paper, a new stereo camera calibration technique that can realize automatic strong calibration is proposed. In order to achieve online camera calibration, an object covered with chess-board patterns, called embedded calibration device, is placed inside the cavity of the stereovision system. We estimate the structural configuration of the embedded calibration device, i.e. the 3D positions of all the grid points on the device, to calibrate the cameras. Since the device is close to the stereo camera, the calibration results are usually not valid for the volume around the object in the scene. Therefore we present a correction approach combining the embedded calibration and scene features to make the calibration valid in the scene. Experimental results demonstrate that our system performs robust and accurate, and is very applicable in unmanned systems.
</div>
<div id="2_bibtex">
@inproceedings{yang2012face,<br/>
&nbsp;&nbsp;author = {Yang, Jiaolong and Liang, Wei and Jia, Yunde},<br/>
&nbsp;&nbsp;title = {Face Pose Estimation with Combined 2D and 3D HOG Features},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 21st International Conference on Pattern Recognition (ICPR)},<br/>
&nbsp;&nbsp;pages = {2492-2495},<br/>
&nbsp;&nbsp;year = {2012}<br/>
}
</div>
</td>
</tr>

</table>

<span style="font-size:14px;font-style:italic;line-height:40px;">Earlier:</span>
<ul>
<li>Xiameng Qin, <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, Wei Liang, Mingtao Pei and Yunde Jia. Stereo Camera Calibration with an Embedded Calibration Device and Scene Features. <span class="jellon_d_i_c_refer">IEEE International Conference on Robotics and Biomimetics (ROBIO)</span>, pp. 2306-2310, 2012.</li>
<li><span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, Lei Chen and Yunde Jia. Human-robot Interaction Technique Based on Stereo Vision. <span class="jellon_d_i_c_refer">Chinese Conference on Human Computer Interaction (CHCI)</span>, pp. 226-231, 2011. (in Chinese)</li>
<li><span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>, Lei Chen and Wei Liang. Monocular Vision based Robot Self-localization. <span class="jellon_d_i_c_refer">IEEE International Conference on Robotics and Biomimetics (ROBIO)</span>, pp. 1189-1193, 2010.</li>
<li>Lei Chen, Mingtao Pei and <span class="jellon_d_i_c_emphasize">Jiaolong Yang</span>. Multi-Scale Matching for Data Association in Vision-based SLAM. <span class="jellon_d_i_c_refer">IEEE International Conference on Robotics and Biomimetics (ROBIO)</span>, pp. 1183-1188, 2010.</li>
</ul>
</div>
</div>
<!-- ITEM END -->

<!-- ITEM BEGIN -->
<div class="jellon_d_item">
<div class="jellon_d_i_title">Academic Services</div>
<div class="jellon_d_i_content">
<i><span class="jellon_d_i_c_emphasize">Conference Area Chair:</span></i><br/> 
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, 2022<br/> 
International Conference on Computer Vision (ICCV), 2021<br/> 
Winter Conference on Applications of Computer Vision (WACV), 2022<br/> 
<i><span class="jellon_d_i_c_emphasize">Conference Program Committee Member/Reviewer:</span></i><br/> 
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, 2016, 2017, 2018, 2019, 2020<br/> 
International Conference on Computer Vision (ICCV), 2015, 2017, 2019<br/> 
European Conference on Computer Vision (ECCV), 2014, 2016, 2018, 2020<br/> 
Conference on Neural Information Processing Systems (NeurIPS), 2019<br/> 
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017<br/> 
ACM International Conferece on Multimedia (MM), 2017<br/>
SIGGRAPH, 2018, 2021 <br/>
<p>
<i><span class="jellon_d_i_c_emphasize">Journal Reviewer: </span></i><br/> 
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br/>
International Journal on Computer Vision (IJCV)<br/>
IEEE Transactions on Image Processing (T-IP)<br/>
IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)<br/>
IEEE Transactions on Robotics (T-RO)<br/>
IEEE Transactions on Multimedia (T-MM)<br/>
IEEE Transactions on Cybernetics (T-CYB)<br/>
IEEE Transactions on Intelligent Transportation Systems (T-ITS)<br/>
IEEE Signal Processing Letters (SPL)<br/>
Computer Vision and Image Understanding (CVIU)<br/>
Machine Vision and Applications (MVA)<br/>
</p>
</div>
</div>
<!-- ITEM END -->

<!-- ITEM BEGIN -->
<!--<div class="jellon_d_item">
<div class="jellon_d_i_title">Selected Awards</div>
<div class="jellon_d_i_content">
<ul>
<li>China Society of Image and Graphics (中国图形图像学会) Excellent PhD Thesis Award, 2017</li>
<li>Excellent BIT PhD Thesis Award (with exceptional award to supervisor, 0.6%), 2016</li>
<li>Excellent Intern Award, Microsoft Research, 2016</li>
<li>ANU Postgraduate Research Scholarship, 2015</li>
<li>Huawei Fellowship, 2014</li>
<li>National Scholarship for Graduate Students, 2013</li>
<li>Chinese Government Scholarship, 2013</li>
<li>National Scholarship for Graduate Students, 2012</li>
</ul>
</div>
</div>-->
<!-- ITEM END -->

<!-- ITEM BEGIN -->
<!--
<div class="jellon_d_item">
<div class="jellon_d_i_title">Teaching & Tutoring</div>
<div class="jellon_d_i_content">
<ul>
<li><span style="display:inline-block;width:85px;">2015-2016</span><span style="display:inline-block;width:90px;">Semester 2</span><span style="display:inline-block;width:240px;">Robotics (<a target="_blank" href="http://programsandcourses.anu.edu.au/course/engn6627">ENGN6627</a>/<a target="_blank" href="http://programsandcourses.anu.edu.au/course/engn4627">ENGN4627</a>)</span><span>ANU</span></li>
<li><span style="display:inline-block;width:85px;">2012-2013</span><span style="display:inline-block;width:90px;">Semester 1</span><span style="display:inline-block;width:240px;">Foundations of Computer Science</span><span>BIT</span></li>
<li><span style="display:inline-block;width:85px;">2011-2012</span><span style="display:inline-block;width:90px;">Semester 2</span><span style="display:inline-block;width:240px;">PHP for the Web</span><span>BIT</span></li>
</ul>

</div>
</div>
-->
<!-- ITEM END -->


<div id="jellon_footer">
<b><i>Declaimer:</i></b> <i>Papers provided on this page are for personal, research use only; copyright and all rights therein are retained by authors and/or other copyright holders.</i><br/>
<b><i>Last update: Aug 1, 2022</i></b><br/>

</div>
</div>

<div style="display:none;">
	<script src="http://s25.cnzz.com/stat.php?id=5490895&web_id=5490895" language="JavaScript"></script>
</div>


</body>
</html>
